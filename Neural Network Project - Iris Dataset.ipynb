{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Neural Nets v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga8ZwCa9x96W",
        "colab_type": "text"
      },
      "source": [
        "# Week 8: Basic Neural Networks\n",
        "\n",
        "by Shivansh Rustagi, Santa Cruz Artificial Intelligence\n",
        "derived from [this blog post](https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/03_numpy_neural_net/Numpy%20deep%20neural%20network.ipynb)\n",
        "\n",
        "Today, we're going to be creating and training a deep neural network. Once it is finished, we'll make some predictons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLALhqXXjkJg",
        "colab_type": "text"
      },
      "source": [
        "First: let's review the steps needed to create a Neural Network:\n",
        "\n",
        "\n",
        "\n",
        "1.   Prepare Data\n",
        "2.   Initialize Neural Network Layers\n",
        "3.   Activation functions (used to normalize outputs of neurons for next layer)\n",
        "4.   Forward Propagation (used for predictions)\n",
        "5.   Loss function (ensures we are moving in the right direction)\n",
        "6.   Backward Propagation (learning and training)\n",
        "7.   Updating parameters (optimize gradients)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZbamUKPkwL1",
        "colab_type": "text"
      },
      "source": [
        "# 1. Prepare Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaiNN3bk1ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons, load_iris\n",
        "\n",
        "# secure the bag\n",
        "iris = load_iris()\n",
        "\n",
        "# since our neural network only has an input dimension of two, instead of using\n",
        "# all four features in the dataset, we will only use petal length and width.\n",
        "# this is why we have d[:2] in the code below\n",
        "\n",
        "# furthermore, since our network uses the Binary Cross Entropy loss function\n",
        "# we can only use two classes.\n",
        "# iris flowers has 50 samples of each of the three classes, so I have simply\n",
        "# chopped off the 50 which belong to the last class\n",
        "# this is why the dataset is limited to the first 100 data using data[:100]\n",
        "X = [d[:2] for d in iris.data[:100]]\n",
        "y = iris.target[:100]\n",
        "\n",
        "# generate a bunch of random linearly separable data for training\n",
        "# this is good because you get as much annotated data as you want\n",
        "# but also it literally doesn't mean anything, it's just a toy set.\n",
        "# to use toy dataset, uncomment this line and change the number of  \n",
        "# epochs to 1000\n",
        "\n",
        "# X, y = make_moons(n_samples = 10000, noise=0.2, random_state=100)\n",
        "\n",
        "# generate our training and testing labels and data!\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8sGK5awk5bb",
        "colab_type": "text"
      },
      "source": [
        "# 2. Initialize Neural Network Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8E-W4dlk9OW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this creates the architecture for our neural network. notice how the \n",
        "# dimension of inputs for layer `n` is equal to the dimensions of the outputs of\n",
        "# layer `n-1`. the final layer has only one output, which is our prediction\n",
        "network_config = [\n",
        "  { 'input_dim': 2, 'output_dim': 25, 'activation': 'relu' },\n",
        "  { 'input_dim': 25, 'output_dim': 50, 'activation': 'relu' },\n",
        "  { 'input_dim': 50, 'output_dim': 50, 'activation': 'relu' },\n",
        "  { 'input_dim': 50, 'output_dim': 25, 'activation': 'relu' },\n",
        "  { 'input_dim': 25, 'output_dim': 1, 'activation': 'sigmoid' }\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHj7ZTabmvSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_layers(config, seed=69):\n",
        "  np.random.seed(seed)\n",
        "  network_num_layers = len(config)\n",
        "  network_params = {}\n",
        "\n",
        "  # enumerate is a python function which takes a dictionary and returns a\n",
        "  # collated representation of the keys:\n",
        "  # eg: {'a': 23, 'b': 42} -> [(0, 'a'), (1, 'b')]\n",
        "  # this makes it easier to iterate over a dictionary, which otherwise does not\n",
        "  # have any order\n",
        "  for idx, layer in enumerate(config):\n",
        "    # number the layers, starting at 1\n",
        "    # YOUR CODE HERE\n",
        "    layer_id = idx + 1\n",
        "    layer_in = layer['input_dim']\n",
        "    layer_out = layer['output_dim']\n",
        "\n",
        "    # the np.random.randn function creates an array prepopulated with values\n",
        "    # sampled from a normal distribution (standard deviation 1, mean 0)\n",
        "    # the size of the array is determined by the arguments passed\n",
        "    \n",
        "    # the two lines of code below are creating randomly initialized weights\n",
        "    # and biases for each layer of our network.\n",
        "    network_params['W' + str(layer_id)] = np.random.randn(layer_out, layer_in) * 0.1\n",
        "    network_params['b' + str(layer_id)] = np.random.randn(layer_out, 1) * 0.1\n",
        "    \n",
        "  return network_params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc2YUBcxmjMG",
        "colab_type": "text"
      },
      "source": [
        "# 3. Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhs8SdI1mpPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sigmoid function: often used as the last activation function in a neural \n",
        "# network, becase it maps numbers to a value between 0 and 1 \n",
        "def sigmoid(Z):\n",
        "  return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "# relu function: negative numbers are mapped to zero, otherwise the value remains the same\n",
        "def relu(Z):\n",
        "  # YOUR CODE HERE\n",
        "  return np.maximum(0, Z)\n",
        "\n",
        "# sigmoid derviative, which we will use during the backpropagation step\n",
        "def sigmoid_derivative(dA, Z):\n",
        "  return dA * sigmoid(Z) * (1 - sigmoid(Z))\n",
        "\n",
        "# relu derviative, which we will use during the backpropagation step\n",
        "def relu_derivative(dA, Z):\n",
        "  dZ = np.array(dA, copy=True)\n",
        "  dZ[Z <= 0] = 0\n",
        "  return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q_CGrVxueo9",
        "colab_type": "text"
      },
      "source": [
        "# 4. Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEYVqXyswD1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# execute the forward propagation step for one layer\n",
        "# this is a helper function for the `full_forward_prop function`\n",
        "def single_layer_forward_prop(A_prev, W, b, activation='relu'):\n",
        "  # matrix multiply the weights with the old activation signal, and add bias\n",
        "  # YOUR CODE HERE\n",
        "  Z = np.dot(W, A_prev) + b\n",
        "  # take this new intermediate result and calculate its activation signal\n",
        "  A = relu(Z) if activation=='relu' else sigmoid(Z)\n",
        "  return A, Z\n",
        "\n",
        "# complete forward propagation step, used to run inferences\n",
        "def full_forward_prop(X, params, config):\n",
        "  # this is temporary \"memory\"/\"cache\" dictionary, which will store, among other\n",
        "  # things, the temporary activation signals and matrices we need to carry out\n",
        "  # backpropagation\n",
        "  # YOUR CODE HERE\n",
        "  temp = {}\n",
        "  # the activation signal for the first layer is simply the input\n",
        "  A = X\n",
        "\n",
        "  # iterate over the layers to carry out forward propagation\n",
        "  for idx, layer in enumerate(config):\n",
        "    layer_id = idx + 1\n",
        "    # get the activation signal from the previous layer\n",
        "    A_prev = A\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # get activation function, weights, and bias for the current layer\n",
        "    activation = layer['activation']\n",
        "    W = params['W' + str(layer_id)]\n",
        "    b = params['b' + str(layer_id)]\n",
        "\n",
        "    # carry out forward propagation for one layer\n",
        "    # YOUR CODE HERE\n",
        "    A, Z = single_layer_forward_prop(A_prev, W, b, activation)\n",
        "\n",
        "    # store the activation signal and intermediate matrix in memory\n",
        "    temp['A' + str(idx)] = A_prev\n",
        "    temp['Z' + str(layer_id)] = Z\n",
        "\n",
        "  return A, temp\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61akOhHP03Zz",
        "colab_type": "text"
      },
      "source": [
        "# 5. Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgN_ayF_0_2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simply a python conversion of the Binary Cross Entropy Loss function\n",
        "# we are using this loss function because it is ideal for choosing between\n",
        "# two classes\n",
        "def cost_fn(y_hat, y):\n",
        "  # YOUR CODE HERE\n",
        "  m = y_hat.shape[1]\n",
        "  cost = -1 / m * (np.dot(y, np.log(y_hat).T) + np.dot(1 - y, np.log(1 - y_hat).T))\n",
        "  # the np.squeeze function gets rid of extra dimensions of an array\n",
        "  # eg: [[[[[1, 2, 3]], [[4, 5, 6]]]]] --> [[1, 2, 3], [4, 5, 6]]\n",
        "  return np.squeeze(cost)\n",
        "\n",
        "# a utility function which takes probabilities and changes them into classes\n",
        "def prob_to_class(p):\n",
        "  p_ = np.copy(p)\n",
        "  p_[p_ > 0.5] = 1\n",
        "  p_[p_ <= 0.5] = 0\n",
        "  return p_\n",
        "\n",
        "# calculate the accuracy\n",
        "def accuracy_fn(y_hat, y):\n",
        "    y_hat_ = prob_to_class(y_hat)\n",
        "    # (y_hat_ == y) returns a matrix of boolean values, comparing each entry\n",
        "    # in both matrices\n",
        "    # once this is done, .all returns the `logical and` across axis 0, and \n",
        "    # computes the mean to give us our value for accuracy\n",
        "    return (y_hat_ == y).all(axis = 0).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDv59c6S7-EL",
        "colab_type": "text"
      },
      "source": [
        "# 6. Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMMfl-X48AJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# backpropagation step for a single layer\n",
        "def single_layer_back_prop(dA, W, b, Z, A_prev, activation=\"relu\"):\n",
        "  # get number of data points\n",
        "  m = A_prev.shape[1]\n",
        "  \n",
        "  # math -> python\n",
        "  dZ = relu_derivative(dA, Z) if activation == 'relu' else sigmoid_derivative(dA, Z)\n",
        "  dW = np.dot(dZ, A_prev.T) / m\n",
        "  db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "  dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "  return dW, db, dA_prev\n",
        "\n",
        "def full_back_prop(y_hat, y, temp, params, config):\n",
        "  # YOUR CODE HERE\n",
        "  # dictionary we will use to calculate gradients\n",
        "  gradients = {}\n",
        "  m = y.shape[1]\n",
        "  # ensure prediction and truth value vectors have same shape\n",
        "  y = y.reshape(y_hat.shape)\n",
        "\n",
        "  # differential of loss with respect to prediction\n",
        "  dA_prev = -(np.divide(y, y_hat) - np.divide(1 - y, 1 - y_hat))\n",
        "\n",
        "  # as implied in the algorithm name, we want to work backward and use what we \n",
        "  # have learned to compute the derivatives of the cost function with respect to\n",
        "  # the weights \n",
        "  for idx, layer in reversed(list(enumerate(config))):\n",
        "    layer_id = idx + 1\n",
        "    activation = layer['activation']\n",
        "\n",
        "    dA = dA_prev\n",
        "\n",
        "    # recover the parameters for a given layer\n",
        "    # YOUR CODE HERE\n",
        "    A_prev = temp[\"A\" + str(idx)]\n",
        "    Z = temp[\"Z\" + str(layer_id)]\n",
        "    W = params[\"W\" + str(layer_id)]\n",
        "    b = params[\"b\" + str(layer_id)]\n",
        "\n",
        "    # carry out a single step of backpropagation on this layer\n",
        "    # YOUR CODE HERE\n",
        "    dW, db, dA_prev = single_layer_back_prop(dA, W, b, Z, A_prev, activation)\n",
        "\n",
        "    # add the gradients we have calculated to be used later\n",
        "    # YOUR CODE HERE\n",
        "    gradients[\"dW\" + str(layer_id)] = dW\n",
        "    gradients[\"db\" + str(layer_id)] = db\n",
        "  \n",
        "  return gradients\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAT5dAgvCjas",
        "colab_type": "text"
      },
      "source": [
        "# 7. Update Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9iW2mzsCmRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the actual learning part: using the gradients we have calculated to update the\n",
        "# weights and biases\n",
        "def update_params(params, config, gradients, learning_rate):\n",
        "  # this time, we want to start the enumeration at 1 because we are moving up\n",
        "  # the layers\n",
        "  for idx, layer in enumerate(config, 1):\n",
        "    # the learning rate is a hyperparameter which controls how fast or slowly\n",
        "    # the parameters are updated (as you can see below, it directly scales by\n",
        "    # how much the gradient is subtracted)\n",
        "    # a super low learning rate could lead to the network never \"converging\"\n",
        "    # and reaching an optimal solution, but a learning rate which is too high\n",
        "    # could lead to DIVERGENCE, which is also very bad\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    params[\"W\" + str(idx)] -= learning_rate * gradients['dW' + str(idx)]\n",
        "    params[\"b\" + str(idx)] -= learning_rate * gradients['db' + str(idx)]\n",
        "\n",
        "  return params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AARF97JtDMUM",
        "colab_type": "text"
      },
      "source": [
        "# Finally... the training step!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmFvkXvrDOfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(X, y, config, epochs, learning_rate):\n",
        "  # create the network\n",
        "  params = init_layers(config)\n",
        "  # collect the cost and accuracy throughout training so that we can plot it if \n",
        "  # we want\n",
        "  cost_history = []\n",
        "  accuracy_history = []\n",
        "\n",
        "  # an epoch is a hyperparameter which siginifies the number of complete passes\n",
        "  # over the training dataset\n",
        "  for i in range(epochs):\n",
        "    # put together everything we wrote up there!\n",
        "    y_hat, mem = full_forward_prop(X, params, config)\n",
        "    \n",
        "    # store accuracy and cost for later\n",
        "    cost_history.append(cost_fn(y_hat, y))\n",
        "    accuracy_history.append(accuracy_fn(y_hat, y))\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    gradients = full_back_prop(y_hat, y, mem, params, config)\n",
        "    params = update_params(params, config, gradients, learning_rate)\n",
        "\n",
        "  # we return the parameters here: aka, the newly trained weights and biases\n",
        "  # for each layer of the network. We can use this set of optimized parameters\n",
        "  # to run inference on new data, which is done below!\n",
        "  return params, cost_history, accuracy_history \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI6j7y4QFG-Q",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SYyGcRoJchv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train this bad boy!\n",
        "params, _, accuracy_history = train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), network_config, epochs=350, learning_rate=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWpqT0Cl7mmV",
        "colab_type": "code",
        "outputId": "184233d6-e777-490b-926a-a5358b8be6fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# quickly just plot accuracy over training\n",
        "plt.plot(accuracy_history)\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.title('accuracy over training period')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrAWJSKTTKhV",
        "colab_type": "code",
        "outputId": "d62bcf99-c943-469e-a007-c4213f7cf750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# run inferences on test data!\n",
        "# remember notation: y_hat is the predicted values, and we predict these values\n",
        "# by sending our test data through the network via the FORWARD PROPAGATION step.\n",
        "y_test_hat, _ = full_forward_prop(np.transpose(X_test), params, network_config)\n",
        "\n",
        "# calculate and print final test accuracy\n",
        "test_accuracy = accuracy_fn(y_test_hat, np.transpose(y_test.reshape((y_test.shape[0], 1))))\n",
        "print(\"Test set accuracy: {:.2f}\".format(test_accuracy))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}